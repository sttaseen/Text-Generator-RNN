{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8babb3",
   "metadata": {},
   "source": [
    "i am gonna take a nap wake me up please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee09fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import operator\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a70eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "# download Book\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef66082a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sttas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a0149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "corpora_dir = \"nltk_data/corpora/state_union\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6bb11a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "\n",
    "print(\"Reading data...\")\n",
    "\n",
    "file_list = []\n",
    "for root, dirs, files in os.walk(corpora_dir):\n",
    "    for filename in files:\n",
    "        file_list.append(os.path.join(root, filename))\n",
    "        \n",
    "sentences = []\n",
    "\n",
    "for files in file_list:\n",
    "    with open(files, 'r') as fin:\n",
    "        try:\n",
    "            str_form = fin.read().replace('\\n', '')\n",
    "            sentences.extend(nltk.sent_tokenize(str_form))\n",
    "        except UnicodeDecodeError:\n",
    "            # some sentences have weird characters. ignore them for now\n",
    "            pass\n",
    "        \n",
    "sentences[:5]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5339c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add start and end sentences\n",
    "sentences = [sentence_start_token +\" \" + x + \" \" + sentence_end_token for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e987e8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af9e5071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['SENTENCE_START', 'PRESIDENT', 'GEORGE', 'H.W', '.', 'SENTENCE_END'],\n",
       " ['SENTENCE_START',\n",
       "  'BUSH',\n",
       "  \"'S\",\n",
       "  'ADDRESS',\n",
       "  'BEFORE',\n",
       "  'A',\n",
       "  'JOINT',\n",
       "  'SESSION',\n",
       "  'OF',\n",
       "  'THE',\n",
       "  'CONGRESS',\n",
       "  'ON',\n",
       "  'THE',\n",
       "  'STATE',\n",
       "  'OF',\n",
       "  'THE',\n",
       "  'UNION',\n",
       "  'January',\n",
       "  '31',\n",
       "  ',',\n",
       "  '1990Mr',\n",
       "  '.',\n",
       "  'SENTENCE_END'],\n",
       " ['SENTENCE_START',\n",
       "  'President',\n",
       "  ',',\n",
       "  'Mr.',\n",
       "  'Speaker',\n",
       "  ',',\n",
       "  'Members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'United',\n",
       "  'States',\n",
       "  'Congress',\n",
       "  ':',\n",
       "  'I',\n",
       "  'return',\n",
       "  'as',\n",
       "  'a',\n",
       "  'former',\n",
       "  'President',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Senate',\n",
       "  'and',\n",
       "  'a',\n",
       "  'former',\n",
       "  'Member',\n",
       "  'of',\n",
       "  'this',\n",
       "  'great',\n",
       "  'House',\n",
       "  '.',\n",
       "  'SENTENCE_END'],\n",
       " ['SENTENCE_START',\n",
       "  'And',\n",
       "  'now',\n",
       "  ',',\n",
       "  'as',\n",
       "  'President',\n",
       "  ',',\n",
       "  'it',\n",
       "  'is',\n",
       "  'my',\n",
       "  'privilege',\n",
       "  'to',\n",
       "  'report',\n",
       "  'to',\n",
       "  'you',\n",
       "  'on',\n",
       "  'the',\n",
       "  'state',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Union',\n",
       "  '.',\n",
       "  'SENTENCE_END'],\n",
       " ['SENTENCE_START',\n",
       "  'Tonight',\n",
       "  'I',\n",
       "  'come',\n",
       "  'not',\n",
       "  'to',\n",
       "  'speak',\n",
       "  'about',\n",
       "  'the',\n",
       "  'state',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Government',\n",
       "  ',',\n",
       "  'not',\n",
       "  'to',\n",
       "  'detail',\n",
       "  'every',\n",
       "  'new',\n",
       "  'initiative',\n",
       "  'we',\n",
       "  'plan',\n",
       "  'for',\n",
       "  'the',\n",
       "  'coming',\n",
       "  'year',\n",
       "  'nor',\n",
       "  'to',\n",
       "  'describe',\n",
       "  'every',\n",
       "  'line',\n",
       "  'in',\n",
       "  'the',\n",
       "  'budget',\n",
       "  '.',\n",
       "  'SENTENCE_END']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the sentence into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "tokenized_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "236aed9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 17514, ',': 16017, 'SENTENCE_START': 12658, 'SENTENCE_END': 12658, '.': 12545, 'of': 11777, 'to': 11021, 'and': 11016, 'in': 6275, 'a': 5336, ...})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd6d2034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 18333  unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"found\", len(word_freq.items()), \" unique words tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fab1a704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 17514),\n",
       " (',', 16017),\n",
       " ('SENTENCE_START', 12658),\n",
       " ('SENTENCE_END', 12658),\n",
       " ('.', 12545)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the most common words and build index_)to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05dfc255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', 'SENTENCE_START', 'SENTENCE_END', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "index_to_word[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5200733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = dict([(w, i) for i,w in enumerate(index_to_word)]) # create word-index map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d3079a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b43737cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qz/5n1l1hd13_n97bsq1g2tntt00000gn/T/ipykernel_2114/2534769696.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
      "/var/folders/qz/5n1l1hd13_n97bsq1g2tntt00000gn/T/ipykernel_2114/2534769696.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n"
     ]
    }
   ],
   "source": [
    "# create the training data\n",
    "# every x represents a word. Every y represents a word that follows it in the sequence\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "238ee764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10th sentence input and expected output to every neuron looks like this\n",
      "\n",
      "[('SENTENCE_START', 'The'), ('The', 'events'), ('events', 'of'), ('of', 'the'), ('the', 'year'), ('year', 'just'), ('just', 'ended'), ('ended', ','), (',', 'the'), ('the', 'Revolution'), ('Revolution', 'of'), ('of', \"'89\"), (\"'89\", ','), (',', 'have'), ('have', 'been'), ('been', 'a'), ('a', 'chain'), ('chain', 'reaction'), ('reaction', ','), (',', 'changes'), ('changes', 'so'), ('so', 'striking'), ('striking', 'that'), ('that', 'it'), ('it', 'marks'), ('marks', 'the'), ('the', 'beginning'), ('beginning', 'of'), ('of', 'a'), ('a', 'new'), ('new', 'era'), ('era', 'in'), ('in', 'the'), ('the', 'world'), ('world', \"'s\"), (\"'s\", 'affairs'), ('affairs', '.'), ('.', 'SENTENCE_END')]\n"
     ]
    }
   ],
   "source": [
    "# print a training data example\n",
    "x_example, y_example = X_train[10], y_train[10]\n",
    "print(\"The 10th sentence input and expected output to every neuron looks like this\\n\")\n",
    "print(list(zip([index_to_word[x] for x in x_example], [index_to_word[y] for y in y_example])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42f8aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "747834a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, word_dim, hidden_dim=50, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate # What is this btt_truncate variable?\n",
    "        # Randomly initialise the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hidden states in s because we need them later.\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        \n",
    "        s = np.zeros((T+1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T ,self.word_dim))\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            # Note that we are indexing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o,s]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "        # For each sentence...\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            # We only care about our prediction of the \"correct\" words\n",
    "            correct_word_predictions = o[np.arange(len(y[i])), y[i]] # FIND HOW THIS IS WORKING!!!!\n",
    "            # Add to the loss based on how off we were\n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    "    \n",
    "    def calculate_loss(self, x, y):\n",
    "        # Divide the total loss by the number of training examples\n",
    "        N = sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x,y)/N\n",
    "    \n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        # Calculate the gradients\n",
    "        dLdU, dLdV, dLdW = self.bptt(x,y)\n",
    "        # Change parameters according to gradients and learning rate\n",
    "        self.U -= learning_rate*dLdU\n",
    "        self.V -= learning_rate*dLdV\n",
    "        self.W -= learning_rate*dLdW\n",
    "        \n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # perform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # We accumulate the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1\n",
    "        # For each output backwards...\n",
    "        for t in np.arange(T)[::-1]: #removed [::-1]\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            # Initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1-(s[t]**2))\n",
    "            # Backpropagation through time (for most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]: #removed ::-1\n",
    "                dLdW+= np.outer(delta_t, s[bptt_step-1])\n",
    "                dLdU[:, x[bptt_step]] += delta_t\n",
    "                # Update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t)  * (1-s[bptt_step-1]**2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold = 0.01):\n",
    "        # Calculate the gradients using backpropagation. We want to chekc if these are ocrret.\n",
    "        bptt_gradients = model.bptt(x, y)\n",
    "        # List of all parameters we want to check.\n",
    "        model_parameters = ['U', 'V', 'W']\n",
    "        # Gradient check for each parameter\n",
    "        for pidx, pname in enumerate(model_parameters):\n",
    "            # Get the actual parameter vaue from the model, eg.g. model.W\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print(\"Performing gradient check for parameter \", pname, \" with size \", np.prod(parameter.shape))\n",
    "            # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                # Save the original value so we can reset it later\n",
    "                original_value = parameter[ix]\n",
    "                # Estimate the gradient using (f(x+h)-f(x-h))/(2*h)\n",
    "                parameter[ix] = original_value+h\n",
    "                gradplus = model.calculate_total_loss([x], [y])\n",
    "                parameter[ix] = original_value-h\n",
    "                gradminus = model.calculate_total_loss([x], [y])\n",
    "                estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "                # Reset paramter to original value\n",
    "                parameter[ix] = original_value\n",
    "                # The gradient for this paramter calculated using backpropagation\n",
    "                backprop_gradient = bptt_gradients[pidx][ix]\n",
    "                # Calculate the relative error: (|x-y|/(|x| + |y|))\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                # If the error is too large, fail the gradient check\n",
    "                if relative_error > error_threshold:\n",
    "                    print(\"Gradient Check ERROR: parameter = \", pname, \", ix= \", ix)\n",
    "                    print(\"+h Loss: \", gradplus)\n",
    "                    print(\"-h Loss: \", gradminus)\n",
    "                    print(\"Estimated gradient: \", estimated_gradient)\n",
    "                    print(\"Backpropagation gradient: \", backprop_gradient)\n",
    "                    print(\"Relative Error: \", relative_error)\n",
    "                    return\n",
    "                it.iternext()\n",
    "            print(\"Gradient check for paramter \", pname, \" passed.\")\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3facb63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "model = RNN(vocabulary_size)\n",
    "\n",
    "o, s = model.forward_propagation(X_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5eade4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38,)\n",
      "[7292 4039 3589  901 6592 2329 7975 6558 7692 3906 5087 5764 4781 4610\n",
      "  324 5786 5442 6732 3092 6925 3256 6906 7391 5494 4698 2280 6752 7857\n",
      "  901 6388 3594 5962 6992 1678 4803 3563 1641 4872]\n"
     ]
    }
   ],
   "source": [
    "# Proving the predit function works\n",
    "predictions = model.predict(X_train[10])\n",
    "print(predictions.shape)\n",
    "# Gibberish output. We're predicitng the next words before even training\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b611cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Part', 'fortitude', '600', 'White', 'unforgettable', 'records', 'Kansas', 'hinder', 'resolving', 'hardships', 'conclusions', 'attaining', 'ink', 'peace.The', 'common', '31.5', 'wonders', 'Parents', 'weight', 'entrusted', 'ten', 'safeguarded', 'Meehan', 'Illinois', 'coast', 'remind', 'representing', 'diet', 'White', 'vetoed', 'Houses', 'Peacekeeper', 'malignant', 'case', 'presidents', 'monopolistic', 'sake', 'conciliation']\n"
     ]
    }
   ],
   "source": [
    "print([index_to_word[w] for w in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "004acb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions:  8.987196820661973\n",
      "Actual Loss:  8.987210753011611\n"
     ]
    }
   ],
   "source": [
    "# Limit to 1000 examples to save time\n",
    "print(\"Expected Loss for random predictions: \", np.log(vocabulary_size))\n",
    "print(\"Actual Loss: \", model.calculate_loss(X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53a3f950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter  U  with size  1000\n",
      "Gradient check for paramter  U  passed.\n",
      "Performing gradient check for parameter  V  with size  1000\n",
      "Gradient check for paramter  V  passed.\n",
      "Performing gradient check for parameter  W  with size  100\n",
      "Gradient check for paramter  W  passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qz/5n1l1hd13_n97bsq1g2tntt00000gn/T/ipykernel_2114/2730553653.py:108: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n"
     ]
    }
   ],
   "source": [
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNN(grad_check_vocab_size, 10, bptt_truncate = 1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f9c137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        if (epoch%evaluate_loss_after ==0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(time, \": Loss after num_examples = \", num_examples_seen, \" epoch=\", epoch, \": \", loss)\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) >1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print(\"Setting learning rate to\" , learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            #One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eccb122d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-30 13:46:53 : Loss after num_examples =  0  epoch= 0 :  8.987205042689844\n",
      "2022-09-30 14:39:53 : Loss after num_examples =  63290  epoch= 5 :  5.200657865975696\n",
      "2022-09-30 15:35:19 : Loss after num_examples =  126580  epoch= 10 :  4.991528359019283\n",
      "2022-09-30 16:27:49 : Loss after num_examples =  189870  epoch= 15 :  4.87035982668855\n",
      "2022-09-30 17:22:48 : Loss after num_examples =  253160  epoch= 20 :  4.783977639345779\n",
      "2022-09-30 18:16:25 : Loss after num_examples =  316450  epoch= 25 :  4.7317962399187055\n",
      "2022-09-30 19:11:07 : Loss after num_examples =  379740  epoch= 30 :  4.688329660993014\n",
      "2022-09-30 20:04:14 : Loss after num_examples =  443030  epoch= 35 :  4.6571169892065845\n",
      "2022-09-30 20:56:58 : Loss after num_examples =  506320  epoch= 40 :  4.628637275695024\n",
      "2022-09-30 21:50:16 : Loss after num_examples =  569610  epoch= 45 :  4.604413737548026\n",
      "2022-09-30 22:45:09 : Loss after num_examples =  632900  epoch= 50 :  4.605459862756691\n",
      "Setting learning rate to 0.0025\n",
      "2022-09-30 23:40:04 : Loss after num_examples =  696190  epoch= 55 :  4.456819905510568\n",
      "2022-10-01 00:37:42 : Loss after num_examples =  759480  epoch= 60 :  4.439986636683652\n",
      "2022-10-01 01:40:49 : Loss after num_examples =  822770  epoch= 65 :  4.4261552205780905\n",
      "2022-10-01 02:38:56 : Loss after num_examples =  886060  epoch= 70 :  4.414008479412255\n",
      "2022-10-01 03:34:48 : Loss after num_examples =  949350  epoch= 75 :  4.403013152951261\n",
      "2022-10-01 04:31:13 : Loss after num_examples =  1012640  epoch= 80 :  4.392945595261447\n",
      "2022-10-01 05:28:41 : Loss after num_examples =  1075930  epoch= 85 :  4.383579284751312\n",
      "2022-10-01 06:24:47 : Loss after num_examples =  1139220  epoch= 90 :  4.374859948244525\n",
      "2022-10-01 07:21:43 : Loss after num_examples =  1202510  epoch= 95 :  4.366703302556971\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNN(vocabulary_size)\n",
    "train_with_sgd(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4f76abf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until we get an end token\n",
    "    while(not new_sentence[-1] ==  word_to_index[sentence_end_token]):\n",
    "        next_word_probs, states = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        \n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    "    \n",
    "\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea36e845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We 'll be the unity of America is to make out our strength our fair address in history while no victory in the recession , but it would be no doubt more that can destroy the resources we can not be .\n",
      "Because we can work , freedom of her treasures Islamic energies and flourish to achieve so to their free , democratic people to speak to make that generations for the more strength for all of themselves , not come for them here in my country , or whose people 's uniform have lost freedom to reform my friend , one day when any failure in one parts of the new posterity order be encouraged on the border to the world that principle that comes from chemical school until you did n't make the dream that a unforgettable renewed proposal for Saudi soul .\n",
      "And I think we can not be to be an America 's belongs question by the Mississippi conflict , the elected industry has already made times of us the American people have n't ever been spending income in the night and very much lesson : responding they need for your own security for the doors of all the days of this postwar history , can be easy issue .\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 3\n",
    "senten_min_length = \n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(mm)\n",
    "    print(\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00fc11b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = []\n",
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebdc4a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(outfile, model):\n",
    "    U,V,W = model.U, model.V, model.W\n",
    "    np.savez(outfile, U=U, V=V, W=W)\n",
    "    print(\"Saved model parameters to: \", outfile)\n",
    "    \n",
    "def load_model(path, model):\n",
    "    npzfile = np.load(path)\n",
    "    U, V, W = npzfile[\"U\"], npzfile[\"V\"], npzfile[\"W\"]\n",
    "    model.hidden_dim = U.shape[0]\n",
    "    model.word_dim = U.shape[1]\n",
    "    model.U = U\n",
    "    model.V = V\n",
    "    model.W = W\n",
    "    print(\"Loaded model paramters from \", path, \". hidden_dim = \", U.shape[0], \" word_dim = \", U.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0116cf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model parameters to:  learned_model\n"
     ]
    }
   ],
   "source": [
    "save_model(\"learned_model\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ea5982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model paramters from  learned_model.npz . hidden_dim =  50  word_dim =  8000\n"
     ]
    }
   ],
   "source": [
    "mm = RNN(vocabulary_size)\n",
    "load_model(\"learned_model.npz\", mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e7d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_sgd(mm, X_train, y_train) # Continue training where we left off"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
